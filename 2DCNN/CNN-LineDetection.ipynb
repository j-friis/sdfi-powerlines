{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0acaca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import laspy\n",
    "#import open3d as o3d\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df25c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPathRelations(full_path_to_data):        \n",
    "    ground_removed_image_paths = []\n",
    "    laz_point_cloud_paths = []\n",
    "        \n",
    "    # Find full path to all images\n",
    "    for path in glob.glob(full_path_to_data+'/ImagesGroundRemovedLarge/*'):\n",
    "        ground_removed_image_paths.append(path)\n",
    "    \n",
    "    # Find full path to all laz files\n",
    "    for path in glob.glob(full_path_to_data+'/LazFilesWithHeightRemoved/*'):\n",
    "        laz_point_cloud_paths.append(path)\n",
    "            \n",
    "    ground_removed_image_paths.sort()\n",
    "    laz_point_cloud_paths.sort()\n",
    "    assert(len(ground_removed_image_paths)==len(laz_point_cloud_paths))\n",
    "    return ground_removed_image_paths, laz_point_cloud_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344bc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxMinNormalize(arr):\n",
    "    return (arr - np.min(arr))/(np.max(arr)-np.min(arr))\n",
    "\n",
    "def CastAllXValuesToImage(arr, x_pixels):\n",
    "    return (MaxMinNormalize(arr))*x_pixels\n",
    "\n",
    "def CastAllYValuesToImage(arr, y_pixels):\n",
    "    return (1-MaxMinNormalize(arr))*y_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8437e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b7573",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4095\n",
      "4095\n",
      "97 97\n",
      "11 11\n"
     ]
    }
   ],
   "source": [
    "all_path_relations = GetPathRelations(\"/home/frederik/data/TestData/data\")\n",
    "path_tuples = list(zip(*all_path_relations))\n",
    "\n",
    "# Normalize to -1 and 1\n",
    "transform_img_gray = transforms.Compose(\n",
    "    [transforms.Resize((4096,4096)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "random_crop_transform = transforms.RandomCrop((128, 128))\n",
    "\n",
    "amount_of_random_images = 100\n",
    "\n",
    "data = []\n",
    "for path in path_tuples[:2]:\n",
    "    image_path, laz_path = path\n",
    "    \n",
    "    # Image to training set\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    image = np.where(image >= 0, image, 0)\n",
    "    image = image/np.max(image)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    \n",
    "    x_pixels, y_pixels = image.shape\n",
    "\n",
    "    # Generate labels \n",
    "    las = laspy.read(laz_path, laz_backend=laspy.compression.LazBackend.LazrsParallel)\n",
    "\n",
    "    # Generate Labels\n",
    "    y_values = CastAllXValuesToImage(las.X, x_pixels).astype(np.int32)\n",
    "    x_values = CastAllYValuesToImage(las.Y, y_pixels).astype(np.int32)\n",
    "\n",
    "    powerline_mask = (las.classification == 14)\n",
    "    x_powerline_values = x_values[powerline_mask]\n",
    "    x_powerline_values = np.where(x_powerline_values < x_pixels, x_powerline_values, x_pixels-1)\n",
    "    x_powerline_values = np.where(x_powerline_values >= 0, x_powerline_values, 0)\n",
    "    \n",
    "    y_powerline_values = y_values[powerline_mask]\n",
    "    y_powerline_values = np.where(y_powerline_values < y_pixels, y_powerline_values, y_pixels-1)\n",
    "    y_powerline_values = np.where(y_powerline_values >= 0, y_powerline_values, 0)\n",
    "\n",
    "    labels = np.zeros((x_pixels, y_pixels)).astype(np.uint8)\n",
    "    for i in range(len(x_powerline_values)):\n",
    "        labels[x_powerline_values[i], y_powerline_values[i]] = 255\n",
    "\n",
    "    # Create kernel\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    lines_image = cv2.dilate(labels, kernel, iterations=1)\n",
    "\n",
    "    # Create Pil Image\n",
    "    lines_image = Image.fromarray(lines_image)\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    random_rotation_values = np.random.uniform(low=2, high=358, size=amount_of_random_images)\n",
    "    for random_rotation_value in random_rotation_values:\n",
    "        image_rotated = transform_img_gray(image.rotate(random_rotation_value, expand=False))\n",
    "        lines_image_rotated = transform_img_gray(lines_image.rotate(random_rotation_value, expand=False))\n",
    "        stacked_for_cropping = torch.stack((image_rotated, lines_image_rotated), dim=0)\n",
    "        data.append(stacked_for_cropping)\n",
    "    \n",
    "# # Randomly shuffle all of the images\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Randomly select 10% as test images.\n",
    "#percent = 10\n",
    "#test_data_size = (1/100)*percent\n",
    "#mask = int(np.floor(len(data)*test_data_size))\n",
    "\n",
    "# X_test = []\n",
    "# Y_test = []\n",
    "# for stacked in data[:mask]:\n",
    "#     img, lab = stacked[0], stacked[1]\n",
    "#     X_test.append(img)\n",
    "#     Y_test.append(lab)\n",
    "\n",
    "trainingImages = []\n",
    "trainingLabels = []\n",
    "\n",
    "random_crops = 1000\n",
    "for stacked in data:\n",
    "    cropped_images = [random_crop_transform(stacked) for _ in range(random_crops)]\n",
    "    for i in cropped_images:\n",
    "        img, lab = i[0], i[1]\n",
    "        # dont add completely black images to training data\n",
    "        if not torch.all(img == -1):\n",
    "            trainingImages.append(img)\n",
    "            trainingLabels.append(lab)\n",
    "\n",
    "# for stacked in data[mask:]:\n",
    "#     cropped_images = [random_crop_transform(stacked) for _ in range(random_crops)]\n",
    "#     for i in cropped_images:\n",
    "#         img, lab = i[0], i[1]\n",
    "#         # dont add completely black images to training data\n",
    "#         if not torch.all(img == -1):\n",
    "#             trainingImages.append(img)\n",
    "#             trainingLabels.append(lab)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(trainingImages, trainingLabels, test_size=0.1)\n",
    "\n",
    "print(len(X_train), len(Y_train))\n",
    "#print(len(X_test), len(Y_test))\n",
    "print(len(X_val), len(Y_val))\n",
    "\n",
    "# Setting up sets for trainlodader and validation loader\n",
    "training_set = []\n",
    "for i in range(len(X_train)):\n",
    "    training_set.append([X_train[i], Y_train[i]])\n",
    "    \n",
    "validation_set = []\n",
    "for i in range(len(Y_val)):\n",
    "    validation_set.append([X_val[i], Y_val[i]])\n",
    "\n",
    "#test_set = []\n",
    "#for i in range(len(Y_test)):\n",
    "#    test_set.append([X_test[i], Y_test[i]])\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_set, batch_size=1, shuffle=True, num_workers=4)\n",
    "valloader = torch.utils.data.DataLoader(validation_set, batch_size=1, shuffle=True, num_workers=4)\n",
    "#testloader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46a1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(valloader, \"valloader.pt\")\n",
    "torch.save(testloader, \"testloader.pt\")\n",
    "#torch.save(trainloader, \"trainloader.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91417df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.load(\"trainloader.pt\")\n",
    "valloader = torch.load(\"valloader.pt\")\n",
    "testloader = torch.load(\"testloader.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0444885",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in labelImages:\n",
    "    if torch.any(i==1):\n",
    "        count +=1\n",
    "\n",
    "print(\"The amount of training images containing powerline: \", count/len(labelImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d71def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886afb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f81351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "    \n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class ConvUNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #\"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(1, 64).cuda()\n",
    "        self.e2 = encoder_block(64, 128).cuda()\n",
    "        self.e3 = encoder_block(128, 256).cuda()\n",
    "        self.e4 = encoder_block(256, 512).cuda()\n",
    "        #\"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024).cuda()\n",
    "        #\"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512).cuda()\n",
    "        self.d2 = decoder_block(512, 256).cuda()\n",
    "        self.d3 = decoder_block(256, 128).cuda()\n",
    "        self.d4 = decoder_block(128, 64).cuda()\n",
    "        #\"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0).cuda()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #\"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        #\"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        #\"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        #\"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d90c2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNetTraining(trainloader, valloader, Conv, lossFunction, learning_rate, epochs):\n",
    "    model = Conv\n",
    "    num_epochs = epochs\n",
    "    criterion = lossFunction.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    # loss arrays for figures\n",
    "    TrainingLossArray = []\n",
    "    ValidationLossArray = []\n",
    "\n",
    "    early_stopping = 50\n",
    "    notImproved = 0\n",
    "    bestLoss = None\n",
    "    bestModel = None\n",
    "    validation_size = len(valloader.dataset)\n",
    "    training_size = len(trainloader.dataset)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        model.train()\n",
    "        print(f\"LR = {scheduler.get_last_lr()[0]:.10f}\")\n",
    "        running_loss = 0.0\n",
    "        for j, data in enumerate(trainloader):\n",
    "            # get the input\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize            \n",
    "            outputs = model(inputs.cuda())\n",
    "            loss = criterion(outputs.cuda(), labels.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Appending the mean running loss\n",
    "        TrainingLossArray.append(running_loss/(j+1))\n",
    "\n",
    "        # print(\"Training loss: \", running_loss/j)\n",
    "        # Finding validation loss\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, data in enumerate(valloader):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                #Calculates loss\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = criterion(outputs.cuda(), labels.cuda())\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        # Appending the mean validation loss\n",
    "        ValidationLossArray.append(validation_loss/validation_size)\n",
    "        print(f\"epoch = {epoch}, Validation loss: {validation_loss/validation_size:.10f}, Training loss: {running_loss/training_size:.10f}\")\n",
    "\n",
    "        # Initialising params for early stopping\n",
    "        if bestLoss == None:\n",
    "            bestLoss = validation_loss\n",
    "\n",
    "        # Checks for early stopping        \n",
    "        if validation_loss <= bestLoss:\n",
    "            notImproved = 0\n",
    "            bestLoss = validation_loss\n",
    "            bestModel = model\n",
    "            torch.save(bestModel.state_dict(), \"bestModelStateDict.pth\")\n",
    "        else:\n",
    "            notImproved +=1\n",
    "        # Converges if the training has not improved for a certain amount of iterations\n",
    "        if notImproved >= early_stopping:\n",
    "            break\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(bestModel.state_dict(), \"lastModelStateDict.pth\")\n",
    "    return bestModel, ValidationLossArray, TrainingLossArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eefdc2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR = 0.0010000000\n",
      "epoch = 0, Validation loss: 0.0448268113, Training loss: 1.0546782517\n",
      "LR = 0.0009500000\n",
      "epoch = 1, Validation loss: 0.0386386823, Training loss: 0.0644065472\n",
      "LR = 0.0009025000\n",
      "epoch = 2, Validation loss: 0.0377913350, Training loss: 0.0642708993\n",
      "LR = 0.0008573750\n",
      "epoch = 3, Validation loss: 0.0366895799, Training loss: 0.0624506978\n",
      "LR = 0.0008145062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8621/3312088251.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbestModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationLossArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingLossArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNetTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# with open('valLoss.npy', 'wb') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     np.save(f, np.array(ValidationLossArray))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8621/1244371025.py\u001b[0m in \u001b[0;36mConvNetTraining\u001b[0;34m(trainloader, valloader, Conv, lossFunction, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Appending the mean running loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bestModel, ValidationLossArray, TrainingLossArray = ConvNetTraining(trainloader, valloader, ConvUNet(), nn.MSELoss(), 0.001, 1000)\n",
    "\n",
    "# with open('valLoss.npy', 'wb') as f:\n",
    "#     np.save(f, np.array(ValidationLossArray))\n",
    "\n",
    "# with open('trainLoss.npy', 'wb') as f:\n",
    "#     np.save(f, np.array(TrainingLossArray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539eab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5311b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfdbe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85ed3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280f496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bfe5fc19ee440272b50e27189dca9d766ee16bd940e6c96fe401988e2293299"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
